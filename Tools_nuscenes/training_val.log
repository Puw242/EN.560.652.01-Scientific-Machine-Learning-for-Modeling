
MLP based method： （Curve 1）
(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ python /home/frank/Pu/sci_ML/Tools_nuscenes/Training_PIBO_XGBoost_v2.py
[Info] Loaded CSV with 60 rows from /home/frank/Pu/sci_ML/nuscenses/top_by_model/merged_top_samples_top10det.csv
[Info] Built dataset: X.shape = (60, 27), y.shape = (60,)
[Split] Train: 48, Val: 12

====================== XGBoost Training ======================
Iter |  Train mlogloss  |  Val mlogloss  |  Val Acc
--------------------------------------------------------------
   1 |           1.0546 |           1.0625 |   0.7500
   2 |           1.0052 |           1.0233 |   0.8333
   3 |           0.9663 |           0.9976 |   0.8333
   4 |           0.9284 |           0.9646 |   0.8333
   5 |           0.8944 |           0.9409 |   0.8333
   6 |           0.8583 |           0.9071 |   0.8333
   7 |           0.8221 |           0.8784 |   0.8333
   8 |           0.7934 |           0.8551 |   0.8333
   9 |           0.7622 |           0.8314 |   0.8333
  10 |           0.7376 |           0.8145 |   0.8333
  11 |           0.7140 |           0.7994 |   0.9167
  12 |           0.6897 |           0.7784 |   0.9167
  13 |           0.6661 |           0.7707 |   0.8333
  14 |           0.6411 |           0.7532 |   0.8333
  15 |           0.6174 |           0.7345 |   0.8333
  16 |           0.5955 |           0.7211 |   0.8333
  17 |           0.5752 |           0.6999 |   0.8333
  18 |           0.5563 |           0.6911 |   0.8333
  19 |           0.5374 |           0.6744 |   0.8333
  20 |           0.5199 |           0.6632 |   0.8333
  21 |           0.5022 |           0.6517 |   0.8333
  22 |           0.4858 |           0.6409 |   0.8333
  23 |           0.4735 |           0.6307 |   0.8333
  24 |           0.4613 |           0.6220 |   0.8333
  25 |           0.4504 |           0.6128 |   0.8333
  26 |           0.4358 |           0.6000 |   0.8333
  27 |           0.4226 |           0.5885 |   0.8333
  28 |           0.4104 |           0.5756 |   0.8333
  29 |           0.3997 |           0.5693 |   0.8333
  30 |           0.3890 |           0.5623 |   0.8333
  31 |           0.3784 |           0.5557 |   0.8333
  32 |           0.3701 |           0.5445 |   0.8333
  33 |           0.3601 |           0.5395 |   0.8333
  34 |           0.3495 |           0.5279 |   0.8333
  35 |           0.3414 |           0.5231 |   0.8333
  36 |           0.3321 |           0.5171 |   0.8333
  37 |           0.3242 |           0.5097 |   0.8333
  38 |           0.3164 |           0.5068 |   0.8333
  39 |           0.3070 |           0.4953 |   0.8333
  40 |           0.3000 |           0.4930 |   0.8333
  41 |           0.2926 |           0.4851 |   0.8333
  42 |           0.2861 |           0.4809 |   0.8333
  43 |           0.2786 |           0.4789 |   0.8333
  44 |           0.2717 |           0.4719 |   0.8333
  45 |           0.2649 |           0.4653 |   0.8333
  46 |           0.2590 |           0.4620 |   0.8333
  47 |           0.2534 |           0.4567 |   0.8333
  48 |           0.2482 |           0.4574 |   0.8333
  49 |           0.2414 |           0.4547 |   0.8333
  50 |           0.2361 |           0.4513 |   0.8333
  51 |           0.2308 |           0.4463 |   0.8333
  52 |           0.2271 |           0.4436 |   0.8333
  53 |           0.2226 |           0.4412 |   0.8333
  54 |           0.2187 |           0.4366 |   0.8333
  55 |           0.2137 |           0.4331 |   0.8333
  56 |           0.2095 |           0.4331 |   0.8333
  57 |           0.2051 |           0.4315 |   0.8333
  58 |           0.2007 |           0.4277 |   0.8333
  59 |           0.1972 |           0.4235 |   0.8333
  60 |           0.1936 |           0.4191 |   0.8333
  61 |           0.1898 |           0.4160 |   0.8333
  62 |           0.1861 |           0.4152 |   0.8333
  63 |           0.1837 |           0.4132 |   0.8333
  64 |           0.1807 |           0.4126 |   0.8333
  65 |           0.1773 |           0.4110 |   0.8333
  66 |           0.1739 |           0.4102 |   0.8333
  67 |           0.1711 |           0.4059 |   0.8333
  68 |           0.1677 |           0.4055 |   0.8333
  69 |           0.1642 |           0.4033 |   0.8333
  70 |           0.1617 |           0.4004 |   0.8333
  71 |           0.1596 |           0.3968 |   0.8333
  72 |           0.1568 |           0.3928 |   0.8333
  73 |           0.1545 |           0.3898 |   0.8333
  74 |           0.1528 |           0.3877 |   0.8333
  75 |           0.1503 |           0.3874 |   0.8333
  76 |           0.1473 |           0.3871 |   0.8333
  77 |           0.1451 |           0.3847 |   0.8333
  78 |           0.1434 |           0.3821 |   0.8333
  79 |           0.1409 |           0.3824 |   0.8333
  80 |           0.1385 |           0.3771 |   0.8333
  81 |           0.1366 |           0.3738 |   0.8333
  82 |           0.1347 |           0.3718 |   0.8333
  83 |           0.1332 |           0.3709 |   0.8333
  84 |           0.1312 |           0.3707 |   0.8333
  85 |           0.1293 |           0.3710 |   0.8333
  86 |           0.1275 |           0.3689 |   0.8333
  87 |           0.1259 |           0.3664 |   0.8333
  88 |           0.1245 |           0.3617 |   0.8333
  89 |           0.1230 |           0.3588 |   0.8333
  90 |           0.1213 |           0.3589 |   0.8333
  91 |           0.1198 |           0.3596 |   0.8333
  92 |           0.1181 |           0.3561 |   0.8333
  93 |           0.1166 |           0.3556 |   0.8333
  94 |           0.1151 |           0.3522 |   0.8333
  95 |           0.1139 |           0.3492 |   0.8333
  96 |           0.1127 |           0.3483 |   0.9167
  97 |           0.1116 |           0.3484 |   0.8333
  98 |           0.1099 |           0.3494 |   0.8333
  99 |           0.1088 |           0.3453 |   0.8333
 100 |           0.1076 |           0.3431 |   0.9167
 101 |           0.1062 |           0.3433 |   0.8333
 102 |           0.1050 |           0.3450 |   0.8333
 103 |           0.1037 |           0.3445 |   0.8333
 104 |           0.1028 |           0.3424 |   0.8333
 105 |           0.1017 |           0.3401 |   0.8333
 106 |           0.1007 |           0.3406 |   0.8333
 107 |           0.0995 |           0.3401 |   0.8333
 108 |           0.0982 |           0.3408 |   0.8333
 109 |           0.0970 |           0.3423 |   0.8333
 110 |           0.0965 |           0.3403 |   0.8333
 111 |           0.0956 |           0.3392 |   0.8333
 112 |           0.0946 |           0.3386 |   0.8333
 113 |           0.0941 |           0.3373 |   0.9167
 114 |           0.0936 |           0.3385 |   0.8333
 115 |           0.0926 |           0.3360 |   0.9167
 116 |           0.0920 |           0.3343 |   0.9167
 117 |           0.0913 |           0.3332 |   0.9167
 118 |           0.0907 |           0.3324 |   0.9167
 119 |           0.0902 |           0.3311 |   0.9167
 120 |           0.0894 |           0.3300 |   0.9167
 121 |           0.0890 |           0.3280 |   0.9167
 122 |           0.0884 |           0.3263 |   0.9167
 123 |           0.0877 |           0.3250 |   0.9167
 124 |           0.0872 |           0.3232 |   0.9167
 125 |           0.0864 |           0.3221 |   0.9167
 126 |           0.0857 |           0.3214 |   0.9167
 127 |           0.0850 |           0.3198 |   0.9167
 128 |           0.0845 |           0.3207 |   0.9167
 129 |           0.0840 |           0.3200 |   0.9167
 130 |           0.0834 |           0.3182 |   0.9167
 131 |           0.0831 |           0.3173 |   0.9167
 132 |           0.0825 |           0.3164 |   0.9167
 133 |           0.0817 |           0.3135 |   0.9167
 134 |           0.0812 |           0.3124 |   0.9167
 135 |           0.0806 |           0.3110 |   0.9167
 136 |           0.0802 |           0.3099 |   0.9167
 137 |           0.0797 |           0.3083 |   0.9167
 138 |           0.0793 |           0.3075 |   0.9167
 139 |           0.0788 |           0.3073 |   0.9167
 140 |           0.0784 |           0.3065 |   0.9167
 141 |           0.0780 |           0.3059 |   0.9167
 142 |           0.0775 |           0.3058 |   0.9167
 143 |           0.0769 |           0.3042 |   0.9167
 144 |           0.0764 |           0.3035 |   0.9167
 145 |           0.0760 |           0.3016 |   0.9167
 146 |           0.0757 |           0.3018 |   0.9167
 147 |           0.0754 |           0.3017 |   0.9167
 148 |           0.0750 |           0.3017 |   0.9167
 149 |           0.0748 |           0.3018 |   0.9167
 150 |           0.0745 |           0.3026 |   0.9167
 151 |           0.0742 |           0.3013 |   0.9167
 152 |           0.0739 |           0.3003 |   0.9167
 153 |           0.0734 |           0.3001 |   0.9167
 154 |           0.0732 |           0.3005 |   0.9167
 155 |           0.0730 |           0.2994 |   0.9167
 156 |           0.0727 |           0.2998 |   0.9167
 157 |           0.0723 |           0.2994 |   0.9167
 158 |           0.0721 |           0.2986 |   0.9167
 159 |           0.0718 |           0.2977 |   0.9167
 160 |           0.0715 |           0.2963 |   0.9167
 161 |           0.0713 |           0.2951 |   0.9167
 162 |           0.0710 |           0.2949 |   0.9167
 163 |           0.0707 |           0.2934 |   0.9167
 164 |           0.0703 |           0.2916 |   0.9167
 165 |           0.0701 |           0.2916 |   0.9167
 166 |           0.0699 |           0.2907 |   0.9167
 167 |           0.0697 |           0.2892 |   0.9167
 168 |           0.0693 |           0.2888 |   0.9167
 169 |           0.0691 |           0.2887 |   0.9167
 170 |           0.0689 |           0.2876 |   0.9167
 171 |           0.0687 |           0.2863 |   0.9167
 172 |           0.0684 |           0.2861 |   0.9167
 173 |           0.0681 |           0.2864 |   0.9167
 174 |           0.0680 |           0.2859 |   0.9167
 175 |           0.0677 |           0.2841 |   0.9167
 176 |           0.0675 |           0.2836 |   0.9167
 177 |           0.0672 |           0.2820 |   0.9167
 178 |           0.0670 |           0.2823 |   0.9167
 179 |           0.0667 |           0.2809 |   0.9167
 180 |           0.0665 |           0.2804 |   0.9167
 181 |           0.0663 |           0.2800 |   0.9167
 182 |           0.0661 |           0.2792 |   0.9167
 183 |           0.0659 |           0.2795 |   0.9167
 184 |           0.0658 |           0.2796 |   0.9167
 185 |           0.0655 |           0.2790 |   0.9167
 186 |           0.0653 |           0.2792 |   0.9167
 187 |           0.0651 |           0.2785 |   0.9167
 188 |           0.0647 |           0.2778 |   0.9167
 189 |           0.0646 |           0.2767 |   0.9167
 190 |           0.0644 |           0.2765 |   0.9167
 191 |           0.0641 |           0.2760 |   0.9167
 192 |           0.0639 |           0.2763 |   0.9167
 193 |           0.0637 |           0.2763 |   0.9167
 194 |           0.0635 |           0.2762 |   0.9167
 195 |           0.0633 |           0.2759 |   0.9167
 196 |           0.0630 |           0.2742 |   0.9167
 197 |           0.0629 |           0.2741 |   0.9167
 198 |           0.0627 |           0.2745 |   0.9167
 199 |           0.0626 |           0.2737 |   0.9167
 200 |           0.0625 |           0.2730 |   0.9167
 201 |           0.0623 |           0.2728 |   0.9167
 202 |           0.0621 |           0.2724 |   0.9167
 203 |           0.0619 |           0.2724 |   0.9167
 204 |           0.0617 |           0.2725 |   0.9167
 205 |           0.0615 |           0.2723 |   0.9167
 206 |           0.0614 |           0.2715 |   0.9167
 207 |           0.0612 |           0.2707 |   0.9167
 208 |           0.0610 |           0.2710 |   0.9167
 209 |           0.0609 |           0.2702 |   0.9167
 210 |           0.0608 |           0.2695 |   0.9167
 211 |           0.0606 |           0.2701 |   0.9167
 212 |           0.0604 |           0.2703 |   0.9167
 213 |           0.0603 |           0.2704 |   0.9167
 214 |           0.0601 |           0.2712 |   0.9167
 215 |           0.0599 |           0.2712 |   0.9167
 216 |           0.0597 |           0.2699 |   0.9167
 217 |           0.0595 |           0.2699 |   0.9167
 218 |           0.0594 |           0.2701 |   0.9167
 219 |           0.0592 |           0.2701 |   0.9167
 220 |           0.0591 |           0.2700 |   0.9167
 221 |           0.0589 |           0.2704 |   0.9167
 222 |           0.0587 |           0.2707 |   0.9167
 223 |           0.0586 |           0.2705 |   0.9167
 224 |           0.0586 |           0.2703 |   0.9167
 225 |           0.0584 |           0.2708 |   0.9167
 226 |           0.0583 |           0.2703 |   0.9167
 227 |           0.0581 |           0.2697 |   0.9167
 228 |           0.0580 |           0.2692 |   0.9167
 229 |           0.0578 |           0.2687 |   0.9167
 230 |           0.0577 |           0.2683 |   0.9167
 231 |           0.0575 |           0.2677 |   0.9167
 232 |           0.0574 |           0.2680 |   0.9167
 233 |           0.0572 |           0.2683 |   0.9167
 234 |           0.0570 |           0.2682 |   0.9167
 235 |           0.0569 |           0.2667 |   0.9167
 236 |           0.0567 |           0.2659 |   0.9167
 237 |           0.0566 |           0.2661 |   0.9167
 238 |           0.0565 |           0.2659 |   0.9167
 239 |           0.0564 |           0.2656 |   0.9167
 240 |           0.0562 |           0.2656 |   0.9167
 241 |           0.0561 |           0.2647 |   0.9167
 242 |           0.0560 |           0.2640 |   0.9167
 243 |           0.0559 |           0.2637 |   0.9167
 244 |           0.0557 |           0.2638 |   0.9167
 245 |           0.0556 |           0.2647 |   0.9167
 246 |           0.0555 |           0.2654 |   0.9167
 247 |           0.0554 |           0.2654 |   0.9167
 248 |           0.0553 |           0.2657 |   0.9167
 249 |           0.0552 |           0.2647 |   0.9167
 250 |           0.0550 |           0.2642 |   0.9167
 251 |           0.0549 |           0.2647 |   0.9167
 252 |           0.0548 |           0.2649 |   0.9167
 253 |           0.0548 |           0.2645 |   0.9167
 254 |           0.0547 |           0.2655 |   0.9167
 255 |           0.0545 |           0.2653 |   0.9167
 256 |           0.0544 |           0.2661 |   0.9167
 257 |           0.0542 |           0.2663 |   0.9167
 258 |           0.0541 |           0.2659 |   0.9167
 259 |           0.0541 |           0.2663 |   0.9167
 260 |           0.0540 |           0.2657 |   0.9167
 261 |           0.0539 |           0.2660 |   0.9167
 262 |           0.0538 |           0.2668 |   0.9167
 263 |           0.0537 |           0.2665 |   0.9167
 264 |           0.0536 |           0.2670 |   0.9167
 265 |           0.0536 |           0.2672 |   0.9167
 266 |           0.0535 |           0.2667 |   0.9167
 267 |           0.0534 |           0.2669 |   0.9167
 268 |           0.0533 |           0.2672 |   0.9167
 269 |           0.0532 |           0.2667 |   0.9167
 270 |           0.0532 |           0.2662 |   0.9167
 271 |           0.0531 |           0.2659 |   0.9167
 272 |           0.0531 |           0.2657 |   0.9167
 273 |           0.0530 |           0.2659 |   0.9167
 274 |           0.0530 |           0.2665 |   0.9167
 275 |           0.0529 |           0.2651 |   0.9167
 276 |           0.0528 |           0.2651 |   0.9167
 277 |           0.0527 |           0.2648 |   0.8333
 278 |           0.0527 |           0.2645 |   0.8333
 279 |           0.0526 |           0.2648 |   0.8333
 280 |           0.0526 |           0.2652 |   0.8333
 281 |           0.0525 |           0.2648 |   0.9167
 282 |           0.0524 |           0.2646 |   0.8333
 283 |           0.0524 |           0.2644 |   0.9167
 284 |           0.0523 |           0.2639 |   0.9167
 285 |           0.0522 |           0.2637 |   0.9167
 286 |           0.0522 |           0.2641 |   0.9167
 287 |           0.0522 |           0.2640 |   0.9167
 288 |           0.0521 |           0.2644 |   0.9167
 289 |           0.0520 |           0.2643 |   0.9167
 290 |           0.0519 |           0.2643 |   0.9167
 291 |           0.0518 |           0.2638 |   0.9167
 292 |           0.0517 |           0.2639 |   0.8333
 293 |           0.0516 |           0.2640 |   0.8333
 294 |           0.0516 |           0.2645 |   0.8333
 295 |           0.0515 |           0.2646 |   0.8333
 296 |           0.0515 |           0.2645 |   0.8333
 297 |           0.0514 |           0.2647 |   0.8333
 298 |           0.0513 |           0.2644 |   0.8333
 299 |           0.0513 |           0.2639 |   0.8333
 300 |           0.0512 |           0.2638 |   0.8333

[Result] Val Accuracy: 0.8333

[Classification Report]
              precision    recall  f1-score   support

      bevdal       0.80      1.00      0.89         4
   bevfusion       1.00      0.50      0.67         4
 transfusion       0.80      1.00      0.89         4

    accuracy                           0.83        12
   macro avg       0.87      0.83      0.81        12
weighted avg       0.87      0.83      0.81        12

[Confusion Matrix]
[[4 0 0]
 [1 2 1]
 [0 0 4]]

[Top 20 Feature Importances]
best_nd_score        : 0.1309
maxscore_bicycle     : 0.1129
count_bicycle        : 0.1008
count_truck          : 0.0713
overall_max          : 0.0674
rank_in_model        : 0.0663
maxscore_car         : 0.0473
maxscore_truck       : 0.0447
count_trailer        : 0.0356
max_det_score        : 0.0329
count_traffic_cone   : 0.0326
overall_mean         : 0.0293
count_pedestrian     : 0.0262
count_barrier        : 0.0259
count_motorcycle     : 0.0258
overall_std          : 0.0248
maxscore_barrier     : 0.0244
count_bus            : 0.0235
maxscore_bus         : 0.0234
maxscore_traffic_cone : 0.0209
/home/frank/miniconda3/envs/openpcdet/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [20:03:58] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.
  warnings.warn(smsg, UserWarning)

[Done] XGBoost model saved to: /home/frank/Pu/sci_ML/Tools_nuscenes/xgb_nuscenes_bestmodel_XGBoost.bin
(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ python /home/frank/Pu/sci_ML/Tools_nuscenes/validation.py
[Load Model] /home/frank/Pu/sci_ML/Tools_nuscenes/xgb_nuscenes_bestmodel_XGBoost_v2.bin
[Info] Loaded CSV with 60 rows.
[Info] Built dataset: X.shape = (60, 27), y.shape = (60,)

==================== Validation Result ====================
Accuracy: 0.9667

[Classification Report]
              precision    recall  f1-score   support

      bevdal       0.95      1.00      0.98        20
   bevfusion       1.00      0.90      0.95        20
 transfusion       0.95      1.00      0.98        20

    accuracy                           0.97        60
   macro avg       0.97      0.97      0.97        60
weighted avg       0.97      0.97      0.97        60

[Confusion Matrix]
[[20  0  0]
 [ 1 18  1]
 [ 0  0 20]]

[Saved] Detailed results written to: /home/frank/Pu/sci_ML/Tools_nuscenes/validation_predictions.csv
(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ 


PIBO + XGBoost (No Curve here)

(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ python /home/frank/Pu/sci_ML/Tools_nuscenes/Training_PIBO_XGBoost_v3.py
[Info] Loaded CSV with 60 rows from /home/frank/Pu/sci_ML/nuscenses/top_by_model/merged_top_samples_top10det.csv
[Info] Built dataset: X.shape = (60, 27), y.shape = (60,), w_phys.mean = 1.0000
[Split] Train: 48, Val: 12

========== Stage 0: PIBO (Physical-Information-Based Initialization) ==========
[0]     train-mlogloss:1.05677  val-mlogloss:1.07680
[10]    train-mlogloss:0.71814  val-mlogloss:0.81705
[20]    train-mlogloss:0.50785  val-mlogloss:0.66599
[30]    train-mlogloss:0.38344  val-mlogloss:0.56748
[40]    train-mlogloss:0.29549  val-mlogloss:0.49459
[50]    train-mlogloss:0.23317  val-mlogloss:0.45548
[60]    train-mlogloss:0.19140  val-mlogloss:0.42526
[70]    train-mlogloss:0.16082  val-mlogloss:0.40708
[79]    train-mlogloss:0.13878  val-mlogloss:0.38801

[Stage 0 Result] Val Accuracy: 0.8333
[Stage 0 Confusion Matrix]
[[3 1 0]
 [1 3 0]
 [0 0 4]]

====================== Stage 1: Supervised Fine-tuning ======================
[0]     train-mlogloss:0.13428  val-mlogloss:0.38574
[10]    train-mlogloss:0.11599  val-mlogloss:0.37827
[20]    train-mlogloss:0.10446  val-mlogloss:0.36309
[30]    train-mlogloss:0.09360  val-mlogloss:0.35467
[40]    train-mlogloss:0.08587  val-mlogloss:0.34767
[50]    train-mlogloss:0.08022  val-mlogloss:0.33797
[60]    train-mlogloss:0.07622  val-mlogloss:0.33387
[70]    train-mlogloss:0.07260  val-mlogloss:0.32252
[80]    train-mlogloss:0.06993  val-mlogloss:0.31507
[90]    train-mlogloss:0.06759  val-mlogloss:0.30942
[100]   train-mlogloss:0.06556  val-mlogloss:0.30706
[110]   train-mlogloss:0.06380  val-mlogloss:0.30149
[119]   train-mlogloss:0.06215  val-mlogloss:0.30026

[Stage 1 Result] Val Accuracy: 0.8333

[Stage 1 Classification Report]
              precision    recall  f1-score   support

      bevdal       0.80      1.00      0.89         4
   bevfusion       1.00      0.50      0.67         4
 Transfusion       0.80      1.00      0.89         4

    accuracy                           0.83        12
   macro avg       0.87      0.83      0.81        12
weighted avg       0.87      0.83      0.81        12

[Stage 1 Confusion Matrix]
[[4 0 0]
 [1 2 1]
 [0 0 4]]

[Training Log Summary]
  Stage 0 train mlogloss last: 0.1388
  Stage 0 val   mlogloss last: 0.3880
  Stage 1 train mlogloss last: 0.0622
  Stage 1 val   mlogloss last: 0.3003
/home/frank/miniconda3/envs/openpcdet/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [20:29:42] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.
  warnings.warn(smsg, UserWarning)

[Done] PIBO-XGBoost model (Stage1 booster) saved to: /home/frank/Pu/sci_ML/Tools_nuscenes/xgb_nuscenes_PIBO_booster.bin
(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ 


Training Optimization step: PIBO + XGBoost (Curve 2) the final performance is more stable and the optimization step is more efficiency 

(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ python /home/frank/Pu/sci_ML/Tools_nuscenes/Training_PIBO_XGBoost_v5.py
[Dataset] Loaded 60 samples from /home/frank/Pu/sci_ML/nuscenses/top_by_model/merged_top_samples_top10det.csv
[Info] Built dataset: X.shape = (60, 27), y.shape = (60,), phys.shape = (60, 3)
[Split] Train: 48, Val: 12

========== Stage 0: PIBO (Physical-Information-Based Initialization) ==========
Epoch |  Train L_phys  |  Val CE (proxy)  |  Val Acc (proxy)
---------------------------------------------------------------
    1 |        1.0602 |          1.0774 |      0.5833
    2 |        1.0145 |          1.0440 |      0.6667
    3 |        0.9791 |          1.0213 |      0.5833
    4 |        0.9438 |          0.9958 |      0.7500
    5 |        0.9129 |          0.9727 |      0.7500
    6 |        0.8806 |          0.9420 |      0.6667
    7 |        0.8456 |          0.9143 |      0.7500
    8 |        0.8191 |          0.8923 |      0.7500
    9 |        0.7927 |          0.8662 |      0.8333
   10 |        0.7689 |          0.8489 |      0.8333

====================== Stage 1: Supervised Training ======================
Epoch |  Train CE (±Phys) |   Val CE       |  Val Acc
------------------------------------------------------
    1 |           0.7409 |       0.8267 |    0.8333
    2 |           0.7168 |       0.8076 |    0.8333
    3 |           0.6939 |       0.7920 |    0.9167
    4 |           0.6698 |       0.7683 |    0.9167
    5 |           0.6487 |       0.7584 |    0.9167
    6 |           0.6293 |       0.7453 |    0.9167
    7 |           0.6078 |       0.7317 |    0.9167
    8 |           0.5863 |       0.7163 |    0.9167
    9 |           0.5705 |       0.7003 |    0.9167
   10 |           0.5511 |       0.6819 |    0.9167
   11 |           0.5359 |       0.6735 |    0.9167
   12 |           0.5171 |       0.6599 |    0.9167
   13 |           0.5010 |       0.6486 |    0.9167
   14 |           0.4853 |       0.6380 |    0.9167
   15 |           0.4720 |       0.6253 |    0.9167
   16 |           0.4600 |       0.6136 |    0.9167
   17 |           0.4474 |       0.6072 |    0.9167
   18 |           0.4338 |       0.5939 |    0.9167
   19 |           0.4205 |       0.5877 |    0.9167
   20 |           0.4084 |       0.5816 |    0.9167
   21 |           0.3975 |       0.5716 |    0.9167
   22 |           0.3859 |       0.5585 |    0.9167
   23 |           0.3745 |       0.5500 |    0.9167
   24 |           0.3630 |       0.5422 |    0.9167
   25 |           0.3535 |       0.5397 |    0.9167
   26 |           0.3468 |       0.5334 |    0.9167
   27 |           0.3375 |       0.5262 |    0.9167
   28 |           0.3292 |       0.5200 |    0.9167
   29 |           0.3193 |       0.5157 |    0.8333
   30 |           0.3121 |       0.5058 |    0.8333
   31 |           0.3055 |       0.4978 |    0.8333
   32 |           0.2995 |       0.4903 |    0.9167
   33 |           0.2924 |       0.4840 |    0.9167
   34 |           0.2849 |       0.4807 |    0.9167
   35 |           0.2779 |       0.4722 |    0.9167
   36 |           0.2723 |       0.4656 |    0.9167
   37 |           0.2664 |       0.4620 |    0.9167
   38 |           0.2608 |       0.4571 |    0.9167
   39 |           0.2558 |       0.4474 |    0.9167
   40 |           0.2499 |       0.4409 |    0.9167

[Final Result] Val Accuracy after Stage 1: 0.9167

[Confusion Matrix]
[[4 0 0]
 [1 3 0]
 [0 0 4]]

[Classification Report]
              precision    recall  f1-score   support

      bevdal       0.80      1.00      0.89         4
   bevfusion       1.00      0.75      0.86         4
 transfusion       1.00      1.00      1.00         4

    accuracy                           0.92        12
   macro avg       0.93      0.92      0.92        12
weighted avg       0.93      0.92      0.92        12

/home/frank/miniconda3/envs/openpcdet/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [20:48:41] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.
  warnings.warn(smsg, UserWarning)

[Done] Two-stage XGBoost model saved to: /home/frank/Pu/sci_ML/Tools_nuscenes/xgb_nuscenes_PIBO_two_stage.bin
(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ 


MLP based method： （Curve 3）

(openpcdet) frank@Frank-3080:~/Pu/sci_ML/Tools_nuscenes$ python /home/frank/Pu/sci_ML/Tools_nuscenes/Training.py
[Dataset] Loaded 60 samples from /home/frank/Pu/sci_ML/nuscenses/top_by_model/merged_top_samples_top10det.csv
[Split] Train: 48, Val: 12

========== Stage 0: PIBO (Physical-Information-Based Initialization) ==========
Epoch |  Train L_phys  |  Val CE (proxy)  |  Val Acc (proxy)
---------------------------------------------------------------
    1 |        0.0959 |          6.1329 |      0.5000
    2 |        0.1448 |          9.6109 |      0.5000
    3 |        0.1344 |          8.8366 |      0.5000
    4 |        0.0548 |          9.6308 |      0.5000
    5 |        0.0665 |          9.6874 |      0.2500

====================== Stage 1: Supervised Training ======================
Epoch |  Train CE (±Phys) |   Val CE       |  Val Acc
------------------------------------------------------
    1 |          10.4492 |       5.0023 |    0.2500
    2 |           4.6999 |       3.9644 |    0.2500
    3 |           3.5312 |       2.6242 |    0.5000
    4 |           2.6871 |       1.7520 |    0.2500
    5 |           1.7342 |       2.8772 |    0.2500
    6 |           1.8272 |       1.7367 |    0.1667
    7 |           1.4863 |       1.4794 |    0.3333
    8 |           1.7790 |       1.6736 |    0.2500
    9 |           1.3800 |       1.5684 |    0.2500
   10 |           1.3256 |       1.3917 |    0.2500
   11 |           1.3160 |       1.1229 |    0.5000
   12 |           1.2190 |       1.3547 |    0.2500
   13 |           1.1689 |       1.2381 |    0.3333
   14 |           1.2400 |       1.6277 |    0.2500
   15 |           1.2586 |       1.1592 |    0.3333
   16 |           1.2362 |       1.2218 |    0.3333
   17 |           1.3555 |       1.3529 |    0.1667
   18 |           1.2099 |       1.1711 |    0.2500
   19 |           1.2502 |       1.5143 |    0.2500
   20 |           1.2809 |       1.1420 |    0.3333
   21 |           1.1310 |       1.6531 |    0.2500
   22 |           1.1554 |       1.1370 |    0.3333
   23 |           1.1798 |       1.4096 |    0.2500
   24 |           1.3214 |       1.1218 |    0.1667
   25 |           1.3350 |       1.2209 |    0.2500
   26 |           1.2258 |       1.7554 |    0.2500
   27 |           1.0850 |       1.1458 |    0.3333
   28 |           1.2243 |       1.3544 |    0.1667
   29 |           1.1040 |       1.1393 |    0.2500
   30 |           1.1481 |       1.3212 |    0.1667
   31 |           1.1455 |       1.4995 |    0.1667
   32 |           1.1107 |       1.1083 |    0.4167
   33 |           1.1430 |       1.2046 |    0.1667
   34 |           1.1556 |       1.3151 |    0.1667
   35 |           1.0939 |       1.4175 |    0.1667
   36 |           1.1436 |       1.1539 |    0.2500
   37 |           1.1017 |       1.4274 |    0.1667
   38 |           1.1442 |       1.2588 |    0.3333
   39 |           1.1356 |       1.2710 |    0.2500
   40 |           1.1506 |       1.2759 |    0.2500
   41 |           1.1258 |       1.1147 |    0.3333
   42 |           1.2858 |       1.7426 |    0.1667
   43 |           1.1894 |       1.1499 |    0.3333
   44 |           1.1974 |       1.5669 |    0.2500
   45 |           1.1648 |       1.0890 |    0.5000
   46 |           1.3647 |       1.5323 |    0.3333
   47 |           1.1548 |       1.2016 |    0.3333
   48 |           1.1776 |       1.6639 |    0.2500
   49 |           1.2675 |       1.1114 |    0.5000
   50 |           1.1690 |       1.4069 |    0.2500
